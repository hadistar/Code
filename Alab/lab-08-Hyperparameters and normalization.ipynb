{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyN06zSb5O16B4YW6z+Zazbf"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Hyperparameters - learning rate"],"metadata":{"id":"sIQKwb2V69q_"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iWQZJps_3X6_","executionInfo":{"status":"ok","timestamp":1707348878508,"user_tz":-540,"elapsed":17033,"user":{"displayName":"Hadistar","userId":"15963219392765003171"}},"outputId":"46176063-6131-454f-bbdb-66b1ee0c7bd6"},"outputs":[{"output_type":"stream","name":"stderr","text":["WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.SGD.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/200\n","1/1 [==============================] - 1s 686ms/step - loss: 3.5899 - accuracy: 0.0000e+00 - val_loss: 0.0203 - val_accuracy: 1.0000\n","Epoch 2/200\n","1/1 [==============================] - 0s 39ms/step - loss: 3.2326 - accuracy: 0.0000e+00 - val_loss: 0.0399 - val_accuracy: 1.0000\n","Epoch 3/200\n","1/1 [==============================] - 0s 35ms/step - loss: 2.9014 - accuracy: 0.0000e+00 - val_loss: 0.0754 - val_accuracy: 1.0000\n","Epoch 4/200\n","1/1 [==============================] - 0s 35ms/step - loss: 2.6034 - accuracy: 0.0000e+00 - val_loss: 0.1343 - val_accuracy: 1.0000\n","Epoch 5/200\n","1/1 [==============================] - 0s 33ms/step - loss: 2.3449 - accuracy: 0.0000e+00 - val_loss: 0.2219 - val_accuracy: 1.0000\n","Epoch 6/200\n","1/1 [==============================] - 0s 38ms/step - loss: 2.1295 - accuracy: 0.0000e+00 - val_loss: 0.3378 - val_accuracy: 1.0000\n","Epoch 7/200\n","1/1 [==============================] - 0s 36ms/step - loss: 1.9557 - accuracy: 0.0000e+00 - val_loss: 0.4743 - val_accuracy: 1.0000\n","Epoch 8/200\n","1/1 [==============================] - 0s 36ms/step - loss: 1.8172 - accuracy: 0.0000e+00 - val_loss: 0.6206 - val_accuracy: 1.0000\n","Epoch 9/200\n","1/1 [==============================] - 0s 38ms/step - loss: 1.7060 - accuracy: 0.3333 - val_loss: 0.7670 - val_accuracy: 0.0000e+00\n","Epoch 10/200\n","1/1 [==============================] - 0s 37ms/step - loss: 1.6147 - accuracy: 0.3333 - val_loss: 0.9072 - val_accuracy: 0.0000e+00\n","Epoch 11/200\n","1/1 [==============================] - 0s 35ms/step - loss: 1.5372 - accuracy: 0.5000 - val_loss: 1.0379 - val_accuracy: 0.0000e+00\n","Epoch 12/200\n","1/1 [==============================] - 0s 35ms/step - loss: 1.4696 - accuracy: 0.5000 - val_loss: 1.1579 - val_accuracy: 0.0000e+00\n","Epoch 13/200\n","1/1 [==============================] - 0s 39ms/step - loss: 1.4090 - accuracy: 0.5000 - val_loss: 1.2673 - val_accuracy: 0.0000e+00\n","Epoch 14/200\n","1/1 [==============================] - 0s 37ms/step - loss: 1.3536 - accuracy: 0.5000 - val_loss: 1.3666 - val_accuracy: 0.0000e+00\n","Epoch 15/200\n","1/1 [==============================] - 0s 34ms/step - loss: 1.3022 - accuracy: 0.5000 - val_loss: 1.4569 - val_accuracy: 0.0000e+00\n","Epoch 16/200\n","1/1 [==============================] - 0s 35ms/step - loss: 1.2540 - accuracy: 0.5000 - val_loss: 1.5390 - val_accuracy: 0.0000e+00\n","Epoch 17/200\n","1/1 [==============================] - 0s 146ms/step - loss: 1.2084 - accuracy: 0.5000 - val_loss: 1.6141 - val_accuracy: 0.0000e+00\n","Epoch 18/200\n","1/1 [==============================] - 0s 47ms/step - loss: 1.1650 - accuracy: 0.5000 - val_loss: 1.6829 - val_accuracy: 0.0000e+00\n","Epoch 19/200\n","1/1 [==============================] - 0s 73ms/step - loss: 1.1238 - accuracy: 0.5000 - val_loss: 1.7463 - val_accuracy: 0.0000e+00\n","Epoch 20/200\n","1/1 [==============================] - 0s 57ms/step - loss: 1.0844 - accuracy: 0.5000 - val_loss: 1.8051 - val_accuracy: 0.0000e+00\n","Epoch 21/200\n","1/1 [==============================] - 0s 79ms/step - loss: 1.0467 - accuracy: 0.5000 - val_loss: 1.8600 - val_accuracy: 0.0000e+00\n","Epoch 22/200\n","1/1 [==============================] - 0s 83ms/step - loss: 1.0109 - accuracy: 0.5000 - val_loss: 1.9116 - val_accuracy: 0.0000e+00\n","Epoch 23/200\n","1/1 [==============================] - 0s 104ms/step - loss: 0.9766 - accuracy: 0.5000 - val_loss: 1.9604 - val_accuracy: 0.0000e+00\n","Epoch 24/200\n","1/1 [==============================] - 0s 74ms/step - loss: 0.9441 - accuracy: 0.5000 - val_loss: 2.0070 - val_accuracy: 0.0000e+00\n","Epoch 25/200\n","1/1 [==============================] - 0s 96ms/step - loss: 0.9132 - accuracy: 0.5000 - val_loss: 2.0518 - val_accuracy: 0.0000e+00\n","Epoch 26/200\n","1/1 [==============================] - 0s 81ms/step - loss: 0.8840 - accuracy: 0.5000 - val_loss: 2.0953 - val_accuracy: 0.0000e+00\n","Epoch 27/200\n","1/1 [==============================] - 0s 94ms/step - loss: 0.8565 - accuracy: 0.5000 - val_loss: 2.1377 - val_accuracy: 0.0000e+00\n","Epoch 28/200\n","1/1 [==============================] - 0s 95ms/step - loss: 0.8306 - accuracy: 0.6667 - val_loss: 2.1794 - val_accuracy: 0.0000e+00\n","Epoch 29/200\n","1/1 [==============================] - 0s 123ms/step - loss: 0.8064 - accuracy: 0.6667 - val_loss: 2.2208 - val_accuracy: 0.0000e+00\n","Epoch 30/200\n","1/1 [==============================] - 0s 158ms/step - loss: 0.7838 - accuracy: 0.6667 - val_loss: 2.2620 - val_accuracy: 0.0000e+00\n","Epoch 31/200\n","1/1 [==============================] - 0s 126ms/step - loss: 0.7629 - accuracy: 0.6667 - val_loss: 2.3032 - val_accuracy: 0.0000e+00\n","Epoch 32/200\n","1/1 [==============================] - 0s 77ms/step - loss: 0.7434 - accuracy: 0.6667 - val_loss: 2.3447 - val_accuracy: 0.0000e+00\n","Epoch 33/200\n","1/1 [==============================] - 0s 121ms/step - loss: 0.7255 - accuracy: 0.6667 - val_loss: 2.3865 - val_accuracy: 0.0000e+00\n","Epoch 34/200\n","1/1 [==============================] - 0s 179ms/step - loss: 0.7091 - accuracy: 0.6667 - val_loss: 2.4288 - val_accuracy: 0.0000e+00\n","Epoch 35/200\n","1/1 [==============================] - 0s 168ms/step - loss: 0.6940 - accuracy: 0.6667 - val_loss: 2.4717 - val_accuracy: 0.0000e+00\n","Epoch 36/200\n","1/1 [==============================] - 0s 107ms/step - loss: 0.6803 - accuracy: 0.6667 - val_loss: 2.5150 - val_accuracy: 0.0000e+00\n","Epoch 37/200\n","1/1 [==============================] - 0s 127ms/step - loss: 0.6677 - accuracy: 0.6667 - val_loss: 2.5590 - val_accuracy: 0.0000e+00\n","Epoch 38/200\n","1/1 [==============================] - 0s 119ms/step - loss: 0.6563 - accuracy: 0.6667 - val_loss: 2.6034 - val_accuracy: 0.0000e+00\n","Epoch 39/200\n","1/1 [==============================] - 0s 104ms/step - loss: 0.6460 - accuracy: 0.6667 - val_loss: 2.6484 - val_accuracy: 0.0000e+00\n","Epoch 40/200\n","1/1 [==============================] - 0s 138ms/step - loss: 0.6366 - accuracy: 0.6667 - val_loss: 2.6937 - val_accuracy: 0.0000e+00\n","Epoch 41/200\n","1/1 [==============================] - 0s 88ms/step - loss: 0.6281 - accuracy: 0.6667 - val_loss: 2.7394 - val_accuracy: 0.0000e+00\n","Epoch 42/200\n","1/1 [==============================] - 0s 121ms/step - loss: 0.6203 - accuracy: 0.6667 - val_loss: 2.7854 - val_accuracy: 0.0000e+00\n","Epoch 43/200\n","1/1 [==============================] - 0s 144ms/step - loss: 0.6133 - accuracy: 0.6667 - val_loss: 2.8315 - val_accuracy: 0.0000e+00\n","Epoch 44/200\n","1/1 [==============================] - 0s 113ms/step - loss: 0.6069 - accuracy: 0.6667 - val_loss: 2.8778 - val_accuracy: 0.0000e+00\n","Epoch 45/200\n","1/1 [==============================] - 0s 94ms/step - loss: 0.6010 - accuracy: 0.6667 - val_loss: 2.9240 - val_accuracy: 0.0000e+00\n","Epoch 46/200\n","1/1 [==============================] - 0s 98ms/step - loss: 0.5957 - accuracy: 0.8333 - val_loss: 2.9702 - val_accuracy: 0.0000e+00\n","Epoch 47/200\n","1/1 [==============================] - 0s 156ms/step - loss: 0.5907 - accuracy: 0.8333 - val_loss: 3.0162 - val_accuracy: 0.0000e+00\n","Epoch 48/200\n","1/1 [==============================] - 0s 94ms/step - loss: 0.5862 - accuracy: 0.8333 - val_loss: 3.0620 - val_accuracy: 0.0000e+00\n","Epoch 49/200\n","1/1 [==============================] - 0s 109ms/step - loss: 0.5820 - accuracy: 0.8333 - val_loss: 3.1076 - val_accuracy: 0.0000e+00\n","Epoch 50/200\n","1/1 [==============================] - 0s 102ms/step - loss: 0.5781 - accuracy: 0.8333 - val_loss: 3.1527 - val_accuracy: 0.0000e+00\n","Epoch 51/200\n","1/1 [==============================] - 0s 52ms/step - loss: 0.5745 - accuracy: 0.8333 - val_loss: 3.1975 - val_accuracy: 0.0000e+00\n","Epoch 52/200\n","1/1 [==============================] - 0s 74ms/step - loss: 0.5711 - accuracy: 0.8333 - val_loss: 3.2418 - val_accuracy: 0.0000e+00\n","Epoch 53/200\n","1/1 [==============================] - 0s 81ms/step - loss: 0.5679 - accuracy: 0.8333 - val_loss: 3.2857 - val_accuracy: 0.0000e+00\n","Epoch 54/200\n","1/1 [==============================] - 0s 90ms/step - loss: 0.5649 - accuracy: 0.8333 - val_loss: 3.3290 - val_accuracy: 0.0000e+00\n","Epoch 55/200\n","1/1 [==============================] - 0s 81ms/step - loss: 0.5621 - accuracy: 0.8333 - val_loss: 3.3718 - val_accuracy: 0.0000e+00\n","Epoch 56/200\n","1/1 [==============================] - 0s 64ms/step - loss: 0.5594 - accuracy: 0.8333 - val_loss: 3.4140 - val_accuracy: 0.0000e+00\n","Epoch 57/200\n","1/1 [==============================] - 0s 63ms/step - loss: 0.5569 - accuracy: 0.8333 - val_loss: 3.4557 - val_accuracy: 0.0000e+00\n","Epoch 58/200\n","1/1 [==============================] - 0s 71ms/step - loss: 0.5544 - accuracy: 0.8333 - val_loss: 3.4968 - val_accuracy: 0.0000e+00\n","Epoch 59/200\n","1/1 [==============================] - 0s 74ms/step - loss: 0.5521 - accuracy: 0.8333 - val_loss: 3.5373 - val_accuracy: 0.0000e+00\n","Epoch 60/200\n","1/1 [==============================] - 0s 64ms/step - loss: 0.5499 - accuracy: 0.8333 - val_loss: 3.5772 - val_accuracy: 0.0000e+00\n","Epoch 61/200\n","1/1 [==============================] - 0s 149ms/step - loss: 0.5478 - accuracy: 0.8333 - val_loss: 3.6165 - val_accuracy: 0.0000e+00\n","Epoch 62/200\n","1/1 [==============================] - 0s 62ms/step - loss: 0.5458 - accuracy: 0.8333 - val_loss: 3.6552 - val_accuracy: 0.0000e+00\n","Epoch 63/200\n","1/1 [==============================] - 0s 79ms/step - loss: 0.5438 - accuracy: 0.8333 - val_loss: 3.6933 - val_accuracy: 0.0000e+00\n","Epoch 64/200\n","1/1 [==============================] - 0s 66ms/step - loss: 0.5419 - accuracy: 0.8333 - val_loss: 3.7308 - val_accuracy: 0.0000e+00\n","Epoch 65/200\n","1/1 [==============================] - 0s 62ms/step - loss: 0.5401 - accuracy: 0.8333 - val_loss: 3.7677 - val_accuracy: 0.0000e+00\n","Epoch 66/200\n","1/1 [==============================] - 0s 75ms/step - loss: 0.5384 - accuracy: 0.8333 - val_loss: 3.8040 - val_accuracy: 0.0000e+00\n","Epoch 67/200\n","1/1 [==============================] - 0s 100ms/step - loss: 0.5367 - accuracy: 0.8333 - val_loss: 3.8398 - val_accuracy: 0.0000e+00\n","Epoch 68/200\n","1/1 [==============================] - 0s 109ms/step - loss: 0.5350 - accuracy: 0.8333 - val_loss: 3.8750 - val_accuracy: 0.0000e+00\n","Epoch 69/200\n","1/1 [==============================] - 0s 128ms/step - loss: 0.5334 - accuracy: 0.8333 - val_loss: 3.9096 - val_accuracy: 0.0000e+00\n","Epoch 70/200\n","1/1 [==============================] - 0s 192ms/step - loss: 0.5319 - accuracy: 0.8333 - val_loss: 3.9437 - val_accuracy: 0.0000e+00\n","Epoch 71/200\n","1/1 [==============================] - 0s 123ms/step - loss: 0.5304 - accuracy: 0.8333 - val_loss: 3.9773 - val_accuracy: 0.0000e+00\n","Epoch 72/200\n","1/1 [==============================] - 0s 69ms/step - loss: 0.5289 - accuracy: 0.8333 - val_loss: 4.0104 - val_accuracy: 0.0000e+00\n","Epoch 73/200\n","1/1 [==============================] - 0s 35ms/step - loss: 0.5275 - accuracy: 0.8333 - val_loss: 4.0429 - val_accuracy: 0.0000e+00\n","Epoch 74/200\n","1/1 [==============================] - 0s 38ms/step - loss: 0.5261 - accuracy: 0.8333 - val_loss: 4.0749 - val_accuracy: 0.0000e+00\n","Epoch 75/200\n","1/1 [==============================] - 0s 39ms/step - loss: 0.5248 - accuracy: 0.8333 - val_loss: 4.1065 - val_accuracy: 0.0000e+00\n","Epoch 76/200\n","1/1 [==============================] - 0s 35ms/step - loss: 0.5234 - accuracy: 0.8333 - val_loss: 4.1376 - val_accuracy: 0.0000e+00\n","Epoch 77/200\n","1/1 [==============================] - 0s 36ms/step - loss: 0.5221 - accuracy: 0.8333 - val_loss: 4.1682 - val_accuracy: 0.0000e+00\n","Epoch 78/200\n","1/1 [==============================] - 0s 37ms/step - loss: 0.5209 - accuracy: 0.8333 - val_loss: 4.1984 - val_accuracy: 0.0000e+00\n","Epoch 79/200\n","1/1 [==============================] - 0s 55ms/step - loss: 0.5197 - accuracy: 0.8333 - val_loss: 4.2281 - val_accuracy: 0.0000e+00\n","Epoch 80/200\n","1/1 [==============================] - 0s 40ms/step - loss: 0.5185 - accuracy: 0.8333 - val_loss: 4.2574 - val_accuracy: 0.0000e+00\n","Epoch 81/200\n","1/1 [==============================] - 0s 36ms/step - loss: 0.5173 - accuracy: 0.8333 - val_loss: 4.2862 - val_accuracy: 0.0000e+00\n","Epoch 82/200\n","1/1 [==============================] - 0s 35ms/step - loss: 0.5161 - accuracy: 0.8333 - val_loss: 4.3147 - val_accuracy: 0.0000e+00\n","Epoch 83/200\n","1/1 [==============================] - 0s 35ms/step - loss: 0.5150 - accuracy: 0.8333 - val_loss: 4.3428 - val_accuracy: 0.0000e+00\n","Epoch 84/200\n","1/1 [==============================] - 0s 36ms/step - loss: 0.5139 - accuracy: 0.8333 - val_loss: 4.3704 - val_accuracy: 0.0000e+00\n","Epoch 85/200\n","1/1 [==============================] - 0s 36ms/step - loss: 0.5128 - accuracy: 0.8333 - val_loss: 4.3977 - val_accuracy: 0.0000e+00\n","Epoch 86/200\n","1/1 [==============================] - 0s 42ms/step - loss: 0.5117 - accuracy: 0.8333 - val_loss: 4.4246 - val_accuracy: 0.0000e+00\n","Epoch 87/200\n","1/1 [==============================] - 0s 39ms/step - loss: 0.5107 - accuracy: 0.8333 - val_loss: 4.4512 - val_accuracy: 0.0000e+00\n","Epoch 88/200\n","1/1 [==============================] - 0s 38ms/step - loss: 0.5097 - accuracy: 0.8333 - val_loss: 4.4774 - val_accuracy: 0.0000e+00\n","Epoch 89/200\n","1/1 [==============================] - 0s 52ms/step - loss: 0.5087 - accuracy: 0.8333 - val_loss: 4.5032 - val_accuracy: 0.0000e+00\n","Epoch 90/200\n","1/1 [==============================] - 0s 39ms/step - loss: 0.5077 - accuracy: 0.8333 - val_loss: 4.5288 - val_accuracy: 0.0000e+00\n","Epoch 91/200\n","1/1 [==============================] - 0s 36ms/step - loss: 0.5067 - accuracy: 0.8333 - val_loss: 4.5540 - val_accuracy: 0.0000e+00\n","Epoch 92/200\n","1/1 [==============================] - 0s 39ms/step - loss: 0.5058 - accuracy: 0.8333 - val_loss: 4.5788 - val_accuracy: 0.0000e+00\n","Epoch 93/200\n","1/1 [==============================] - 0s 37ms/step - loss: 0.5048 - accuracy: 0.8333 - val_loss: 4.6034 - val_accuracy: 0.0000e+00\n","Epoch 94/200\n","1/1 [==============================] - 0s 36ms/step - loss: 0.5039 - accuracy: 0.8333 - val_loss: 4.6277 - val_accuracy: 0.0000e+00\n","Epoch 95/200\n","1/1 [==============================] - 0s 36ms/step - loss: 0.5030 - accuracy: 0.8333 - val_loss: 4.6516 - val_accuracy: 0.0000e+00\n","Epoch 96/200\n","1/1 [==============================] - 0s 37ms/step - loss: 0.5021 - accuracy: 0.8333 - val_loss: 4.6753 - val_accuracy: 0.0000e+00\n","Epoch 97/200\n","1/1 [==============================] - 0s 36ms/step - loss: 0.5012 - accuracy: 0.8333 - val_loss: 4.6987 - val_accuracy: 0.0000e+00\n","Epoch 98/200\n","1/1 [==============================] - 0s 42ms/step - loss: 0.5003 - accuracy: 0.8333 - val_loss: 4.7218 - val_accuracy: 0.0000e+00\n","Epoch 99/200\n","1/1 [==============================] - 0s 36ms/step - loss: 0.4995 - accuracy: 0.8333 - val_loss: 4.7447 - val_accuracy: 0.0000e+00\n","Epoch 100/200\n","1/1 [==============================] - 0s 36ms/step - loss: 0.4986 - accuracy: 0.8333 - val_loss: 4.7672 - val_accuracy: 0.0000e+00\n","Epoch 101/200\n","1/1 [==============================] - 0s 36ms/step - loss: 0.4978 - accuracy: 0.8333 - val_loss: 4.7896 - val_accuracy: 0.0000e+00\n","Epoch 102/200\n","1/1 [==============================] - 0s 37ms/step - loss: 0.4970 - accuracy: 0.8333 - val_loss: 4.8117 - val_accuracy: 0.0000e+00\n","Epoch 103/200\n","1/1 [==============================] - 0s 38ms/step - loss: 0.4961 - accuracy: 0.8333 - val_loss: 4.8335 - val_accuracy: 0.0000e+00\n","Epoch 104/200\n","1/1 [==============================] - 0s 39ms/step - loss: 0.4953 - accuracy: 0.8333 - val_loss: 4.8551 - val_accuracy: 0.0000e+00\n","Epoch 105/200\n","1/1 [==============================] - 0s 36ms/step - loss: 0.4946 - accuracy: 0.8333 - val_loss: 4.8764 - val_accuracy: 0.0000e+00\n","Epoch 106/200\n","1/1 [==============================] - 0s 41ms/step - loss: 0.4938 - accuracy: 0.8333 - val_loss: 4.8976 - val_accuracy: 0.0000e+00\n","Epoch 107/200\n","1/1 [==============================] - 0s 37ms/step - loss: 0.4930 - accuracy: 0.8333 - val_loss: 4.9185 - val_accuracy: 0.0000e+00\n","Epoch 108/200\n","1/1 [==============================] - 0s 35ms/step - loss: 0.4922 - accuracy: 0.8333 - val_loss: 4.9391 - val_accuracy: 0.0000e+00\n","Epoch 109/200\n","1/1 [==============================] - 0s 39ms/step - loss: 0.4915 - accuracy: 0.8333 - val_loss: 4.9596 - val_accuracy: 0.0000e+00\n","Epoch 110/200\n","1/1 [==============================] - 0s 37ms/step - loss: 0.4907 - accuracy: 0.8333 - val_loss: 4.9799 - val_accuracy: 0.0000e+00\n","Epoch 111/200\n","1/1 [==============================] - 0s 38ms/step - loss: 0.4900 - accuracy: 0.8333 - val_loss: 4.9999 - val_accuracy: 0.0000e+00\n","Epoch 112/200\n","1/1 [==============================] - 0s 35ms/step - loss: 0.4893 - accuracy: 0.8333 - val_loss: 5.0198 - val_accuracy: 0.0000e+00\n","Epoch 113/200\n","1/1 [==============================] - 0s 64ms/step - loss: 0.4886 - accuracy: 0.8333 - val_loss: 5.0394 - val_accuracy: 0.0000e+00\n","Epoch 114/200\n","1/1 [==============================] - 0s 38ms/step - loss: 0.4878 - accuracy: 0.8333 - val_loss: 5.0589 - val_accuracy: 0.0000e+00\n","Epoch 115/200\n","1/1 [==============================] - 0s 38ms/step - loss: 0.4871 - accuracy: 0.8333 - val_loss: 5.0782 - val_accuracy: 0.0000e+00\n","Epoch 116/200\n","1/1 [==============================] - 0s 37ms/step - loss: 0.4864 - accuracy: 0.8333 - val_loss: 5.0973 - val_accuracy: 0.0000e+00\n","Epoch 117/200\n","1/1 [==============================] - 0s 36ms/step - loss: 0.4858 - accuracy: 0.8333 - val_loss: 5.1162 - val_accuracy: 0.0000e+00\n","Epoch 118/200\n","1/1 [==============================] - 0s 35ms/step - loss: 0.4851 - accuracy: 0.8333 - val_loss: 5.1349 - val_accuracy: 0.0000e+00\n","Epoch 119/200\n","1/1 [==============================] - 0s 37ms/step - loss: 0.4844 - accuracy: 0.8333 - val_loss: 5.1534 - val_accuracy: 0.0000e+00\n","Epoch 120/200\n","1/1 [==============================] - 0s 37ms/step - loss: 0.4837 - accuracy: 0.8333 - val_loss: 5.1718 - val_accuracy: 0.0000e+00\n","Epoch 121/200\n","1/1 [==============================] - 0s 44ms/step - loss: 0.4831 - accuracy: 0.8333 - val_loss: 5.1900 - val_accuracy: 0.0000e+00\n","Epoch 122/200\n","1/1 [==============================] - 0s 37ms/step - loss: 0.4824 - accuracy: 0.8333 - val_loss: 5.2081 - val_accuracy: 0.0000e+00\n","Epoch 123/200\n","1/1 [==============================] - 0s 39ms/step - loss: 0.4818 - accuracy: 0.8333 - val_loss: 5.2259 - val_accuracy: 0.0000e+00\n","Epoch 124/200\n","1/1 [==============================] - 0s 35ms/step - loss: 0.4811 - accuracy: 0.8333 - val_loss: 5.2437 - val_accuracy: 0.0000e+00\n","Epoch 125/200\n","1/1 [==============================] - 0s 36ms/step - loss: 0.4805 - accuracy: 0.8333 - val_loss: 5.2612 - val_accuracy: 0.0000e+00\n","Epoch 126/200\n","1/1 [==============================] - 0s 39ms/step - loss: 0.4799 - accuracy: 0.8333 - val_loss: 5.2787 - val_accuracy: 0.0000e+00\n","Epoch 127/200\n","1/1 [==============================] - 0s 36ms/step - loss: 0.4792 - accuracy: 0.8333 - val_loss: 5.2959 - val_accuracy: 0.0000e+00\n","Epoch 128/200\n","1/1 [==============================] - 0s 36ms/step - loss: 0.4786 - accuracy: 0.8333 - val_loss: 5.3130 - val_accuracy: 0.0000e+00\n","Epoch 129/200\n","1/1 [==============================] - 0s 37ms/step - loss: 0.4780 - accuracy: 0.8333 - val_loss: 5.3300 - val_accuracy: 0.0000e+00\n","Epoch 130/200\n","1/1 [==============================] - 0s 36ms/step - loss: 0.4774 - accuracy: 0.8333 - val_loss: 5.3468 - val_accuracy: 0.0000e+00\n","Epoch 131/200\n","1/1 [==============================] - 0s 37ms/step - loss: 0.4768 - accuracy: 0.8333 - val_loss: 5.3635 - val_accuracy: 0.0000e+00\n","Epoch 132/200\n","1/1 [==============================] - 0s 41ms/step - loss: 0.4762 - accuracy: 0.8333 - val_loss: 5.3801 - val_accuracy: 0.0000e+00\n","Epoch 133/200\n","1/1 [==============================] - 0s 37ms/step - loss: 0.4756 - accuracy: 0.8333 - val_loss: 5.3965 - val_accuracy: 0.0000e+00\n","Epoch 134/200\n","1/1 [==============================] - 0s 36ms/step - loss: 0.4750 - accuracy: 0.8333 - val_loss: 5.4128 - val_accuracy: 0.0000e+00\n","Epoch 135/200\n","1/1 [==============================] - 0s 36ms/step - loss: 0.4744 - accuracy: 0.8333 - val_loss: 5.4289 - val_accuracy: 0.0000e+00\n","Epoch 136/200\n","1/1 [==============================] - 0s 40ms/step - loss: 0.4738 - accuracy: 0.8333 - val_loss: 5.4450 - val_accuracy: 0.0000e+00\n","Epoch 137/200\n","1/1 [==============================] - 0s 45ms/step - loss: 0.4733 - accuracy: 0.8333 - val_loss: 5.4609 - val_accuracy: 0.0000e+00\n","Epoch 138/200\n","1/1 [==============================] - 0s 41ms/step - loss: 0.4727 - accuracy: 0.8333 - val_loss: 5.4766 - val_accuracy: 0.0000e+00\n","Epoch 139/200\n","1/1 [==============================] - 0s 38ms/step - loss: 0.4721 - accuracy: 0.8333 - val_loss: 5.4923 - val_accuracy: 0.0000e+00\n","Epoch 140/200\n","1/1 [==============================] - 0s 39ms/step - loss: 0.4716 - accuracy: 0.8333 - val_loss: 5.5078 - val_accuracy: 0.0000e+00\n","Epoch 141/200\n","1/1 [==============================] - 0s 36ms/step - loss: 0.4710 - accuracy: 0.8333 - val_loss: 5.5232 - val_accuracy: 0.0000e+00\n","Epoch 142/200\n","1/1 [==============================] - 0s 37ms/step - loss: 0.4705 - accuracy: 0.8333 - val_loss: 5.5385 - val_accuracy: 0.0000e+00\n","Epoch 143/200\n","1/1 [==============================] - 0s 36ms/step - loss: 0.4699 - accuracy: 0.8333 - val_loss: 5.5537 - val_accuracy: 0.0000e+00\n","Epoch 144/200\n","1/1 [==============================] - 0s 41ms/step - loss: 0.4694 - accuracy: 0.8333 - val_loss: 5.5688 - val_accuracy: 0.0000e+00\n","Epoch 145/200\n","1/1 [==============================] - 0s 38ms/step - loss: 0.4688 - accuracy: 0.8333 - val_loss: 5.5837 - val_accuracy: 0.0000e+00\n","Epoch 146/200\n","1/1 [==============================] - 0s 37ms/step - loss: 0.4683 - accuracy: 0.8333 - val_loss: 5.5986 - val_accuracy: 0.0000e+00\n","Epoch 147/200\n","1/1 [==============================] - 0s 38ms/step - loss: 0.4678 - accuracy: 0.8333 - val_loss: 5.6133 - val_accuracy: 0.0000e+00\n","Epoch 148/200\n","1/1 [==============================] - 0s 38ms/step - loss: 0.4672 - accuracy: 0.8333 - val_loss: 5.6280 - val_accuracy: 0.0000e+00\n","Epoch 149/200\n","1/1 [==============================] - 0s 42ms/step - loss: 0.4667 - accuracy: 0.8333 - val_loss: 5.6425 - val_accuracy: 0.0000e+00\n","Epoch 150/200\n","1/1 [==============================] - 0s 48ms/step - loss: 0.4662 - accuracy: 0.8333 - val_loss: 5.6569 - val_accuracy: 0.0000e+00\n","Epoch 151/200\n","1/1 [==============================] - 0s 40ms/step - loss: 0.4657 - accuracy: 0.8333 - val_loss: 5.6712 - val_accuracy: 0.0000e+00\n","Epoch 152/200\n","1/1 [==============================] - 0s 37ms/step - loss: 0.4652 - accuracy: 0.8333 - val_loss: 5.6855 - val_accuracy: 0.0000e+00\n","Epoch 153/200\n","1/1 [==============================] - 0s 36ms/step - loss: 0.4647 - accuracy: 0.8333 - val_loss: 5.6996 - val_accuracy: 0.0000e+00\n","Epoch 154/200\n","1/1 [==============================] - 0s 41ms/step - loss: 0.4641 - accuracy: 0.8333 - val_loss: 5.7136 - val_accuracy: 0.0000e+00\n","Epoch 155/200\n","1/1 [==============================] - 0s 37ms/step - loss: 0.4636 - accuracy: 0.8333 - val_loss: 5.7276 - val_accuracy: 0.0000e+00\n","Epoch 156/200\n","1/1 [==============================] - 0s 35ms/step - loss: 0.4631 - accuracy: 0.8333 - val_loss: 5.7414 - val_accuracy: 0.0000e+00\n","Epoch 157/200\n","1/1 [==============================] - 0s 57ms/step - loss: 0.4626 - accuracy: 0.8333 - val_loss: 5.7552 - val_accuracy: 0.0000e+00\n","Epoch 158/200\n","1/1 [==============================] - 0s 44ms/step - loss: 0.4621 - accuracy: 0.8333 - val_loss: 5.7688 - val_accuracy: 0.0000e+00\n","Epoch 159/200\n","1/1 [==============================] - 0s 38ms/step - loss: 0.4617 - accuracy: 0.8333 - val_loss: 5.7824 - val_accuracy: 0.0000e+00\n","Epoch 160/200\n","1/1 [==============================] - 0s 39ms/step - loss: 0.4612 - accuracy: 0.8333 - val_loss: 5.7959 - val_accuracy: 0.0000e+00\n","Epoch 161/200\n","1/1 [==============================] - 0s 50ms/step - loss: 0.4607 - accuracy: 0.8333 - val_loss: 5.8093 - val_accuracy: 0.0000e+00\n","Epoch 162/200\n","1/1 [==============================] - 0s 36ms/step - loss: 0.4602 - accuracy: 0.8333 - val_loss: 5.8226 - val_accuracy: 0.0000e+00\n","Epoch 163/200\n","1/1 [==============================] - 0s 36ms/step - loss: 0.4597 - accuracy: 0.8333 - val_loss: 5.8358 - val_accuracy: 0.0000e+00\n","Epoch 164/200\n","1/1 [==============================] - 0s 41ms/step - loss: 0.4592 - accuracy: 0.8333 - val_loss: 5.8489 - val_accuracy: 0.0000e+00\n","Epoch 165/200\n","1/1 [==============================] - 0s 40ms/step - loss: 0.4588 - accuracy: 0.8333 - val_loss: 5.8620 - val_accuracy: 0.0000e+00\n","Epoch 166/200\n","1/1 [==============================] - 0s 37ms/step - loss: 0.4583 - accuracy: 0.8333 - val_loss: 5.8749 - val_accuracy: 0.0000e+00\n","Epoch 167/200\n","1/1 [==============================] - 0s 39ms/step - loss: 0.4578 - accuracy: 0.8333 - val_loss: 5.8878 - val_accuracy: 0.0000e+00\n","Epoch 168/200\n","1/1 [==============================] - 0s 36ms/step - loss: 0.4574 - accuracy: 0.8333 - val_loss: 5.9006 - val_accuracy: 0.0000e+00\n","Epoch 169/200\n","1/1 [==============================] - 0s 39ms/step - loss: 0.4569 - accuracy: 0.8333 - val_loss: 5.9134 - val_accuracy: 0.0000e+00\n","Epoch 170/200\n","1/1 [==============================] - 0s 38ms/step - loss: 0.4564 - accuracy: 0.8333 - val_loss: 5.9260 - val_accuracy: 0.0000e+00\n","Epoch 171/200\n","1/1 [==============================] - 0s 40ms/step - loss: 0.4560 - accuracy: 0.8333 - val_loss: 5.9386 - val_accuracy: 0.0000e+00\n","Epoch 172/200\n","1/1 [==============================] - 0s 41ms/step - loss: 0.4555 - accuracy: 0.8333 - val_loss: 5.9511 - val_accuracy: 0.0000e+00\n","Epoch 173/200\n","1/1 [==============================] - 0s 39ms/step - loss: 0.4551 - accuracy: 0.8333 - val_loss: 5.9635 - val_accuracy: 0.0000e+00\n","Epoch 174/200\n","1/1 [==============================] - 0s 38ms/step - loss: 0.4546 - accuracy: 0.8333 - val_loss: 5.9759 - val_accuracy: 0.0000e+00\n","Epoch 175/200\n","1/1 [==============================] - 0s 41ms/step - loss: 0.4542 - accuracy: 0.8333 - val_loss: 5.9882 - val_accuracy: 0.0000e+00\n","Epoch 176/200\n","1/1 [==============================] - 0s 38ms/step - loss: 0.4537 - accuracy: 0.8333 - val_loss: 6.0004 - val_accuracy: 0.0000e+00\n","Epoch 177/200\n","1/1 [==============================] - 0s 38ms/step - loss: 0.4533 - accuracy: 0.8333 - val_loss: 6.0125 - val_accuracy: 0.0000e+00\n","Epoch 178/200\n","1/1 [==============================] - 0s 41ms/step - loss: 0.4529 - accuracy: 0.8333 - val_loss: 6.0246 - val_accuracy: 0.0000e+00\n","Epoch 179/200\n","1/1 [==============================] - 0s 39ms/step - loss: 0.4524 - accuracy: 0.8333 - val_loss: 6.0366 - val_accuracy: 0.0000e+00\n","Epoch 180/200\n","1/1 [==============================] - 0s 57ms/step - loss: 0.4520 - accuracy: 0.8333 - val_loss: 6.0485 - val_accuracy: 0.0000e+00\n","Epoch 181/200\n","1/1 [==============================] - 0s 39ms/step - loss: 0.4516 - accuracy: 0.8333 - val_loss: 6.0604 - val_accuracy: 0.0000e+00\n","Epoch 182/200\n","1/1 [==============================] - 0s 41ms/step - loss: 0.4511 - accuracy: 0.8333 - val_loss: 6.0721 - val_accuracy: 0.0000e+00\n","Epoch 183/200\n","1/1 [==============================] - 0s 40ms/step - loss: 0.4507 - accuracy: 0.8333 - val_loss: 6.0839 - val_accuracy: 0.0000e+00\n","Epoch 184/200\n","1/1 [==============================] - 0s 61ms/step - loss: 0.4503 - accuracy: 0.8333 - val_loss: 6.0955 - val_accuracy: 0.0000e+00\n","Epoch 185/200\n","1/1 [==============================] - 0s 37ms/step - loss: 0.4498 - accuracy: 0.8333 - val_loss: 6.1071 - val_accuracy: 0.0000e+00\n","Epoch 186/200\n","1/1 [==============================] - 0s 39ms/step - loss: 0.4494 - accuracy: 0.8333 - val_loss: 6.1187 - val_accuracy: 0.0000e+00\n","Epoch 187/200\n","1/1 [==============================] - 0s 39ms/step - loss: 0.4490 - accuracy: 0.8333 - val_loss: 6.1301 - val_accuracy: 0.0000e+00\n","Epoch 188/200\n","1/1 [==============================] - 0s 41ms/step - loss: 0.4486 - accuracy: 0.8333 - val_loss: 6.1415 - val_accuracy: 0.0000e+00\n","Epoch 189/200\n","1/1 [==============================] - 0s 40ms/step - loss: 0.4482 - accuracy: 0.8333 - val_loss: 6.1529 - val_accuracy: 0.0000e+00\n","Epoch 190/200\n","1/1 [==============================] - 0s 42ms/step - loss: 0.4478 - accuracy: 0.8333 - val_loss: 6.1642 - val_accuracy: 0.0000e+00\n","Epoch 191/200\n","1/1 [==============================] - 0s 40ms/step - loss: 0.4473 - accuracy: 0.8333 - val_loss: 6.1754 - val_accuracy: 0.0000e+00\n","Epoch 192/200\n","1/1 [==============================] - 0s 38ms/step - loss: 0.4469 - accuracy: 0.8333 - val_loss: 6.1866 - val_accuracy: 0.0000e+00\n","Epoch 193/200\n","1/1 [==============================] - 0s 41ms/step - loss: 0.4465 - accuracy: 0.8333 - val_loss: 6.1977 - val_accuracy: 0.0000e+00\n","Epoch 194/200\n","1/1 [==============================] - 0s 41ms/step - loss: 0.4461 - accuracy: 0.8333 - val_loss: 6.2087 - val_accuracy: 0.0000e+00\n","Epoch 195/200\n","1/1 [==============================] - 0s 44ms/step - loss: 0.4457 - accuracy: 0.8333 - val_loss: 6.2197 - val_accuracy: 0.0000e+00\n","Epoch 196/200\n","1/1 [==============================] - 0s 38ms/step - loss: 0.4453 - accuracy: 0.8333 - val_loss: 6.2306 - val_accuracy: 0.0000e+00\n","Epoch 197/200\n","1/1 [==============================] - 0s 41ms/step - loss: 0.4449 - accuracy: 0.8333 - val_loss: 6.2415 - val_accuracy: 0.0000e+00\n","Epoch 198/200\n","1/1 [==============================] - 0s 38ms/step - loss: 0.4445 - accuracy: 0.8333 - val_loss: 6.2523 - val_accuracy: 0.0000e+00\n","Epoch 199/200\n","1/1 [==============================] - 0s 39ms/step - loss: 0.4441 - accuracy: 0.8333 - val_loss: 6.2631 - val_accuracy: 0.0000e+00\n","Epoch 200/200\n","1/1 [==============================] - 0s 38ms/step - loss: 0.4437 - accuracy: 0.8333 - val_loss: 6.2738 - val_accuracy: 0.0000e+00\n","1/1 [==============================] - 0s 42ms/step - loss: 0.0804 - accuracy: 1.0000\n","Accuracy:  1.0\n"]}],"source":["# Lab 7 Learning rate and Evaluation\n","import tensorflow as tf\n","import numpy as np\n","\n","x_data = [[1, 2, 1],\n","          [1, 3, 2],\n","          [1, 3, 4],\n","          [1, 5, 5],\n","          [1, 7, 5],\n","          [1, 2, 5],\n","          [1, 6, 6],\n","          [1, 7, 7]]\n","y_data = [[0, 0, 1],\n","          [0, 0, 1],\n","          [0, 0, 1],\n","          [0, 1, 0],\n","          [0, 1, 0],\n","          [0, 1, 0],\n","          [1, 0, 0],\n","          [1, 0, 0]]\n","\n","# Evaluation our model using this test dataset\n","x_test = [[2, 1, 1],\n","          [3, 1, 2],\n","          [3, 3, 4]]\n","y_test = [[0, 0, 1],\n","          [0, 0, 1],\n","          [0, 0, 1]]\n","\n","# try different learning_rate\n","# learning_rate = 65535  # ? it works too hahaha\n","learning_rate = 1e-10\n","# learning_rate = 1e-10  # small learning rate won't work either\n","\n","tf.model = tf.keras.Sequential()\n","tf.model.add(tf.keras.layers.Dense(units=3, input_dim=3, activation='softmax'))\n","tf.model.compile(loss='categorical_crossentropy', optimizer=tf.keras.optimizers.SGD(lr=learning_rate), metrics=['accuracy'])\n","\n","tf.model.fit(np.array(x_data), np.array(y_data), epochs=200, validation_split=0.2)\n","\n","# predict\n","#print(\"Prediction: \", tf.model.predict_classes(x_test))\n","\n","# Calculate the accuracy\n","print(\"Accuracy: \", tf.model.evaluate(x_test, y_test)[1])\n"]},{"cell_type":"markdown","source":["# linear_regression_without_min_max"],"metadata":{"id":"W_rmkyRv7G1q"}},{"cell_type":"code","source":["import tensorflow as tf\n","import numpy as np\n","\n","xy = np.array([[828.659973, 833.450012, 908100, 828.349976, 831.659973],\n","               [823.02002, 828.070007, 1828100, 821.655029, 828.070007],\n","               [819.929993, 824.400024, 1438100, 818.97998, 824.159973],\n","               [816, 820.958984, 1008100, 815.48999, 819.23999],\n","               [819.359985, 823, 1188100, 818.469971, 818.97998],\n","               [819, 823, 1198100, 816, 820.450012],\n","               [811.700012, 815.25, 1098100, 809.780029, 813.669983],\n","               [809.51001, 816.659973, 1398100, 804.539978, 809.559998]])\n","\n","x_data = xy[:, 0:-1]\n","y_data = xy[:, [-1]]\n","\n","tf.model = tf.keras.Sequential()\n","tf.model.add(tf.keras.layers.Dense(units=1, input_dim=4))\n","tf.model.add(tf.keras.layers.Activation('linear'))\n","tf.model.compile(loss='mse', optimizer=tf.keras.optimizers.SGD(lr=1e-5), metrics=['accuracy'])\n","tf.model.summary()\n","\n","history = tf.model.fit(x_data, y_data, epochs=100)\n","\n","print(history.history['loss']) # loss == nan"],"metadata":{"id":"1iTn1Yw07Aka"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# linear_regression_min_max"],"metadata":{"id":"3rSg1Os97IEF"}},{"cell_type":"code","source":["import tensorflow as tf\n","import numpy as np\n","\n","def min_max_scaler(data):\n","    numerator = data - np.min(data, 0)\n","    denominator = np.max(data, 0) - np.min(data, 0)\n","    # noise term prevents the zero division\n","    return numerator / (denominator + 1e-7)\n","\n","xy = np.array(\n","    [\n","        [828.659973, 833.450012, 908100, 828.349976, 831.659973],\n","        [823.02002, 828.070007, 1828100, 821.655029, 828.070007],\n","        [819.929993, 824.400024, 1438100, 818.97998, 824.159973],\n","        [816       , 820.958984, 1008100, 815.48999, 819.23999],\n","        [819.359985, 823, 1188100, 818.469971, 818.97998],\n","        [819, 823, 1198100, 816, 820.450012],\n","        [811.700012, 815.25, 1098100, 809.780029, 813.669983],\n","        [809.51001, 816.659973, 1398100, 804.539978, 809.559998],\n","    ]\n",")\n","\n","# very important. It does not work without it.\n","xy = min_max_scaler(xy)\n","print(xy)\n","\n","'''\n","[[0.99999999 0.99999999 0.         1.         1.        ]\n"," [0.70548491 0.70439552 1.         0.71881782 0.83755791]\n"," [0.54412549 0.50274824 0.57608696 0.606468   0.6606331 ]\n"," [0.33890353 0.31368023 0.10869565 0.45989134 0.43800918]\n"," [0.51436    0.42582389 0.30434783 0.58504805 0.42624401]\n"," [0.49556179 0.42582389 0.31521739 0.48131134 0.49276137]\n"," [0.11436064 0.         0.20652174 0.22007776 0.18597238]\n"," [0.         0.07747099 0.5326087  0.         0.        ]]\n","'''\n","\n","x_data = xy[:, 0:-1]\n","y_data = xy[:, [-1]]\n","\n","tf.model = tf.keras.Sequential()\n","tf.model.add(tf.keras.layers.Dense(units=1, input_dim=4))\n","tf.model.add(tf.keras.layers.Activation('linear'))\n","tf.model.compile(loss='mse', optimizer=tf.keras.optimizers.SGD(lr=1e-5), metrics=['accuracy'])\n","tf.model.summary()\n","\n","history = tf.model.fit(x_data, y_data, epochs=2000)\n","\n","predictions = tf.model.predict(x_data)\n","score = tf.model.evaluate(x_data, y_data)\n","\n","print('Prediction: \\n', predictions)\n","print('Cost: ', score)\n"],"metadata":{"id":"63MF3-Ty7HYW"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"PpgurhG6Q2q8"},"source":["## 하이퍼 파라미터(Hyper Parameter)\n","\n","- 사람이 직접 설정해야하는 매개변수\n","- 학습이 되기전 미리 설정되어 상수취급"]},{"cell_type":"markdown","metadata":{"id":"MovMTKvLNOJR"},"source":["### 학습률(Learning Rate)\n","- 학습률에 따라 학습정도가 달라짐\n","- 적절한 학습률을 찾는 것이 핵심"]},{"cell_type":"markdown","metadata":{"id":"uU4VwrkLNR_8"},"source":["### 학습 횟수(Epochs)\n","- 학습 횟수를 너무 작게, 또는 너무 크게 지정하면 과소적합 또는 과대적합 발생\n","- 여러번 진행하면서 최적의 학습 횟수(epochs)값을 찾아야함"]},{"cell_type":"markdown","metadata":{"id":"D0CoUCbKNToW"},"source":["### 미니배치 크기(Mini Batch Size)\n","- 미니 배치 학습\n","  - 한번 학습할 때 메모리의 부족현상을 막기 위해 전체 데이터의 일부를 여러번 학습하는 방식\n","- 한번 학습할 때마다 얼마만큼의 미니배치 크기를 사용할지 결정\n","- 배치 크기가 작을수록 학습 시간이 많이 소요되고, 클수록 학습 시간이 학습 시간은 적게 소요된다.  \n","  "]},{"cell_type":"markdown","metadata":{"id":"9jdpc8gdNVt3"},"source":["### 검증데이터(Validation Data)\n","- 주어진 데이터를 학습 + 검증 + 테스트 데이터로 구분하여 과적합을 방지\n","- 일반적으로 전체 데이터의 2~30%를 테스트 데이터, 나머지에서 20%정도를 검증용 데이터, 남은 부분을 학습용 데이터로 사용\n","\n","  <img src=\"https://miro.medium.com/max/1400/1*4G__SV580CxFj78o9yUXuQ.png\" width=\"600\">\n"]}]}